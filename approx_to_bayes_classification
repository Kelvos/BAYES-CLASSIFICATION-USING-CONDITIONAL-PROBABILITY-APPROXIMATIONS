{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from collections import Counter\n",
    "import operator\n",
    "import numpy as np\n",
    "from sklearn.inspection import permutation_importance\n",
    "import csv\n",
    "import itertools\n",
    "import random\n",
    "import warnings as warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class TimeError(Exception):\n",
    "    \"\"\"Custom exception class for timer\"\"\"\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self._start_time = None\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start a new timer\"\"\"\n",
    "        if self._start_time is not None:\n",
    "            raise TimerError(f\"Timer is running. Use .stop() to stop it\")\n",
    "\n",
    "        self._start_time = time.perf_counter()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer, and report the elapsed time\"\"\"\n",
    "        if self._start_time is None:\n",
    "            raise TimerError(f\"Timer is not running. Use .start() to start it\")\n",
    "\n",
    "        elapsed_time = time.perf_counter() - self._start_time\n",
    "        self._start_time = None\n",
    "        return(elapsed_time)\n",
    "#         print(f\"Elapsed time: {elapsed_time:0.4f} seconds\")\n",
    "        \n",
    "\n",
    "\n",
    "class MNBClassifier(ClassifierMixin, BaseEstimator):\n",
    "    \"\"\" \n",
    "    ----------\n",
    "    X_ : ndarray, shape (n_samples, n_features)\n",
    "        The input passed during :meth:`fit`.\n",
    "    y_ : ndarray, shape (n_samples,)\n",
    "        The labels passed during :meth:`fit`.\n",
    "    classes_ : ndarray, shape (n_classes,)\n",
    "        The classes seen at :meth:`fit`.\n",
    "    \"\"\"\n",
    "    def __init__(self, kappa=20):\n",
    "        self.kappa = kappa\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        y : array-like, shape (n_samples,)\n",
    "            The target values. An array of int.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            The label for each sample is the label of the closest sample\n",
    "            seen during fit.\n",
    "        \"\"\"\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self, ['X_', 'y_'])\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        #X is test data\n",
    "        #self.X_ is the training data\n",
    "        \n",
    "        #prior probabilities\n",
    "        #access cth label using z[cth]\n",
    "        #Z are class prior probabilities\n",
    "        y_pred=[]\n",
    "        z = np.zeros((np.shape(self.classes_)[0],1 ))\n",
    "        for i in range(len(self.X_)):\n",
    "            z[int(self.y_[i])] += 1  \n",
    "\n",
    "        z = z/sum(z)\n",
    "            \n",
    "\n",
    "        \n",
    "        R = np.zeros((np.shape(self.X_)[1],1))\n",
    "        for k in range(0, np.shape(self.X_)[1]): \n",
    "            R[k] = np.amax(self.X_[:,k]) - np.amin(self.X_[:,k])\n",
    "        \n",
    "\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            v = np.zeros((np.shape(self.classes_)[0],1))\n",
    "            c = np.zeros((np.shape(self.classes_)[0],1))\n",
    "            p = np.zeros((np.shape(self.classes_)[0],1))\n",
    "            for j in range(len(self.X_)):\n",
    "                d=0\n",
    "                for k in range(0,np.shape(self.X_)[1]):\n",
    "                    d += ((X[i,k] - self.X_[j,k])/R[k])**2\n",
    "                v[int(self.y_[j])] += 1/((1 + math.sqrt(d))**self.kappa)\n",
    "                c[int(self.y_[j])] += 1\n",
    "            p = z*v/c\n",
    "\n",
    "            y_pred.append(np.argmax(p))\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    \n",
    "def load_data(data_file):\n",
    "    df = pd.read_csv('{}'.format(data_file),header=None)\n",
    "#     df = pd.read_csv('datasets3/iris.data.csv'.format(data_file),header=None)\n",
    "    \n",
    "    #UCI has ? as missing data\n",
    "    df = df[~df.eq('?').any(1)]\n",
    "    df.dropna(axis = 1, how ='all', inplace = True)\n",
    "    df.dropna(axis = 0, how ='all', inplace = True)\n",
    "    df.to_csv('cleaned_frame.csv'.format(data_file),index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #reread for appropriate column dtypes\n",
    "    df = pd.read_csv('cleaned_frame.csv',header=0)\n",
    "    result_summary[data_file][\"data_type\"]=Counter(df.dtypes.tolist())\n",
    "    \n",
    "    #convert Y to integer type\n",
    "    df[df.columns[-1]]=df[df.columns[-1]].astype('str')\n",
    "    df[df.columns[-1]]=df[df.columns[-1]].str.strip()\n",
    "    df[df.columns[-1]]=df[df.columns[-1]].astype('category')\n",
    "    df[df.columns[-1]]=df[df.columns[-1]].cat.codes\n",
    "    \n",
    "    #get the columns that needed to be label encoded\n",
    "    conversion_idx=[]\n",
    "#     print(df.dtypes.values)\n",
    "    for idx,d_type in enumerate(df.dtypes.values):\n",
    "        if \"object\" == d_type:\n",
    "            conversion_idx.append(idx)\n",
    "\n",
    "    \n",
    "    \n",
    "    #clean categories of white space\n",
    "    for idx in conversion_idx:\n",
    "        df.iloc[:,idx]=df.iloc[:,idx].astype('str')\n",
    "        df.iloc[:,idx]=df.iloc[:,idx].str.strip()\n",
    "        df.iloc[:,idx]=df.iloc[:,idx].astype('category')\n",
    "    \n",
    "    #encode strings as numbers\n",
    "    for idx in conversion_idx:\n",
    "        labels = df.iloc[:,idx].astype('category').cat.categories.tolist()\n",
    "        replace_map_comp = {idx : {k: v for k,v in zip(labels,list(range(1,len(labels)+1)))}}\n",
    "        df.iloc[:,idx].replace(replace_map_comp[idx], inplace=True)\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "def feature_selection(model,X,y,kf,num_CV,acc,removed_col):\n",
    "\n",
    "    number_of_repeats = 10\n",
    "    rkf = RepeatedKFold(n_splits=num_CV, n_repeats=number_of_repeats, random_state=6920)\n",
    "    #Use state to break while loop if acc decreases. \n",
    "    state = True\n",
    "    important_features={}\n",
    "    highest_score = [0,acc]\n",
    "    print(\"new_loop_initial_scores {}\".format(highest_score))\n",
    "    print(X.columns)\n",
    "    for col in list(X.columns.values):\n",
    "        important_features[col]=0\n",
    "        \n",
    "    for train_index , test_index in rkf.split(X,y):\n",
    "        X_train , X_test = X.iloc[train_index,:],X.iloc[test_index,:]\n",
    "        y_train , y_test = y.iloc[train_index] , y.iloc[test_index]\n",
    "        \n",
    "        \n",
    "        model = model.fit(X_train, y_train)\n",
    "#         r = permutation_importance(model, X_test, y_test,\n",
    "#                            n_repeats=1,\n",
    "#                             scoring= 'accuracy')\n",
    "        r = permutation_importance(model, X_test, y_test)\n",
    "        imp_features=[]\n",
    "#         print(\"importance means {}\".format(r.importances_mean))\n",
    "        for imp_idx,i in enumerate(r.importances_mean):\n",
    "            important_features[list(X.columns.values)[imp_idx]] += i\n",
    "\n",
    "        \n",
    "        y_pred_t = model.fit(X_train, y_train).predict(X_test)\n",
    "        highest_score[0] += (1-((y_test != y_pred_t).sum()/(X_test.shape[0])))/(num_CV*number_of_repeats)\n",
    "    \n",
    "    print(\"Score for this run {}\".format(highest_score[0]))\n",
    "    sorted_features = {k: v for k, v in sorted(important_features.items(), key=lambda item: item[1],reverse = True)}\n",
    "    if highest_score[0]>highest_score[1]:\n",
    "                state =True\n",
    "                print(\"score was higher\")\n",
    "                print(\"features and importance in dict{}\".format(sorted_features))\n",
    "                cols = list(sorted_features.keys())\n",
    "                print(\"all featues for current iteration{}\".format(cols))\n",
    "                if len(cols)>1:\n",
    "                    removed_col = cols.pop()\n",
    "                print(\"featues for next iteration {}\".format(cols))\n",
    "    \n",
    " \n",
    "    else:\n",
    "                state =False\n",
    "                print(\"ending while loop\")\n",
    "                print(\"score was lower\")\n",
    "                print(\"features and importance in dict{}\".format(sorted_features))\n",
    "                cols = list(sorted_features.keys())\n",
    "                print(\"all featues for this iteration {}\".format(cols))\n",
    "                cols.append(removed_col)\n",
    "                print(\"features selected for evaluation {}\".format(cols))\n",
    "    \n",
    "\n",
    "\n",
    "    return (max(highest_score[0],highest_score[1]),cols,state,removed_col)\n",
    "\n",
    "def naive_bayes_analysis(data_file,result_summary={},final_features=[0],kappa=1):\n",
    "    \n",
    "    #set up data collection \n",
    "\n",
    "\n",
    "    result_summary[data_file]={}\n",
    "    result_summary[data_file][\"data_file\"]=data_file\n",
    "    result_summary[data_file][\"Gaussian\"]=0\n",
    "    result_summary[data_file][\"Laplacian\"]=0\n",
    "    result_summary[data_file][\"kNN\"]=0\n",
    "    result_summary[data_file][\"kNN_20\"]=0\n",
    "    result_summary[data_file][\"MNB_optimal\"]=0\n",
    "    result_summary[data_file][\"MNB_optimal_kappa\"]=[]\n",
    "    \n",
    "    result_summary[data_file][\"MNB_20\"] = 0\n",
    "    result_summary[data_file][\"MNB_60\"] = 0\n",
    "    result_summary[data_file][\"MNB_time\"]=0\n",
    "    result_summary[data_file][\"Laplace_time\"]=0\n",
    "    result_summary[data_file][\"MNB_optimal_kappa\"].append(kappa)\n",
    "    \n",
    "    #feature selection\n",
    "    number_of_repeats = 10\n",
    "    num_CV= 10\n",
    "    acc = 0\n",
    "    removed_column = 10000\n",
    "    kf = KFold(num_CV,shuffle=True,random_state=6920)\n",
    "    rkf = RepeatedKFold(n_splits=num_CV, n_repeats=number_of_repeats, random_state=6920)\n",
    "    #data cleaning happens in load data\n",
    "    df = load_data(data_file)\n",
    "    \n",
    "    #separate into x|Y\n",
    "    X = df[df.columns[0:len(df.columns)-1]]\n",
    "    y= df[df.columns[-1]]\n",
    "    print(X.columns)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    result_summary[data_file][\"selected_features\"]=final_features\n",
    "    X = df[df.columns[0:len(df.columns)-1]].loc[:,final_features]\n",
    "            \n",
    "            \n",
    "    labels = y.unique()\n",
    "    \n",
    "    \n",
    "    # determine hyperparameters\n",
    "\n",
    "    params_grid={'n_neighbors':range(1,100,1)}\n",
    "    search_KNN = GridSearchCV(KNeighborsClassifier(), param_grid=params_grid,\n",
    "                              n_jobs=-1,cv=kf,scoring='accuracy').fit(X, y)\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"{}-NN\".format(search_KNN.best_params_['n_neighbors']))\n",
    "\n",
    "\n",
    "\n",
    "    t = Timer()\n",
    "\n",
    "    for train_index , test_index in rkf.split(X,y):\n",
    "        X_train , X_test = X.iloc[train_index,:],X.iloc[test_index,:]\n",
    "        y_train , y_test = y.iloc[train_index] , y.iloc[test_index]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #KNN\n",
    "\n",
    "        neigh = KNeighborsClassifier(n_neighbors=search_KNN.best_params_['n_neighbors'])\n",
    "        y_kNN_pred = neigh.fit(X_train, y_train).predict(X_test)\n",
    "        result_summary[data_file][\"kNN\"]+=(1-((y_test != y_kNN_pred).sum()/(X_test.shape[0])))/(num_CV*number_of_repeats)\n",
    "\n",
    "\n",
    "        #KNN_20\n",
    "\n",
    "        neigh = KNeighborsClassifier(20)\n",
    "        y_kNN_pred = neigh.fit(X_train, y_train).predict(X_test)\n",
    "        result_summary[data_file][\"kNN_20\"]+=(1-((y_test != y_kNN_pred).sum()/(X_test.shape[0])))/(num_CV*number_of_repeats)\n",
    "\n",
    "\n",
    "        #Gaussian Naive Bayes\n",
    "\n",
    "        t.start()\n",
    "        gnb = GaussianNB()\n",
    "        y_GNB_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "        result_summary[data_file][\"Gaussian\"]+=(1-((y_test != y_GNB_pred).sum()/(X_test.shape[0])))/(num_CV*number_of_repeats)\n",
    "        t.stop()\n",
    "\n",
    "#       \n",
    "        #approximated bayesian classification\n",
    "\n",
    "\n",
    "        t.start()\n",
    "        MNB = MNBClassifier(kappa)\n",
    "        y_MNB_pred = MNB.fit(X_train, y_train).predict(X_test)\n",
    "        result_summary[data_file][\"MNB_time\"]=t.stop()/num_CV\n",
    "        result_summary[data_file][\"MNB_optimal\"]+=(1-((y_test != y_MNB_pred).sum()/(X_test.shape[0])))/(num_CV*number_of_repeats)\n",
    "\n",
    "\n",
    "        MNB = MNBClassifier(20)\n",
    "        y_MNB_pred = MNB.fit(X_train, y_train).predict(X_test)\n",
    "        result_summary[data_file][\"MNB_20\"]+=(1-((y_test != y_MNB_pred).sum()/(X_test.shape[0])))/(num_CV*number_of_repeats)\n",
    "\n",
    "        MNB = MNBClassifier(60)\n",
    "        y_MNB_pred = MNB.fit(X_train, y_train).predict(X_test)\n",
    "        result_summary[data_file][\"MNB_60\"]+=(1-((y_test != y_MNB_pred).sum()/(X_test.shape[0])))/(num_CV*number_of_repeats)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#             Laplacian\n",
    "        prob={}\n",
    "        y_MNB = []\n",
    "        t.start()\n",
    "        Z={}\n",
    "        for label in labels:\n",
    "            Z[label] = (y_train.values == label).sum()/y.shape[0]\n",
    "    #     print(\"Z {}\".format(Z))\n",
    "\n",
    "\n",
    "        for i,row in X_test.iterrows():\n",
    "            row = row.tolist()\n",
    "            class_sample_size = 0\n",
    "            zero_freq=0\n",
    "\n",
    "            #create prob dict\n",
    "            for label in labels:\n",
    "                prob[label] = 0\n",
    "\n",
    "            for k, col in enumerate(X_train.columns):\n",
    "                #casting to avoid iloc str error\n",
    "                col = int(col)\n",
    "                hosein_estimate = 1\n",
    "                v = 10\n",
    "                class_sample_size={}\n",
    "                x_col = X_train.iloc[:,k]\n",
    "\n",
    "\n",
    "                occ_score={}\n",
    "                for label in labels:\n",
    "                    occ_score[label] = 0\n",
    "                    class_sample_size[label] = x_col[y_train==label].shape[0]\n",
    "\n",
    "                for label in labels:\n",
    "                    v = (x_col[y_train==label].values == row[k]).sum()\n",
    "\n",
    "                    if (v == 0):\n",
    "                        occ_score[label] = 1\n",
    "                    else:\n",
    "                        occ_score[label] = v + 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                for label in labels:\n",
    "                    prob[label] += math.log(occ_score[label]/(class_sample_size[label]+len(x_col.unique())))\n",
    "\n",
    "\n",
    "            for label in labels:\n",
    "                prob[label] += math.log(Z[label])\n",
    "\n",
    "\n",
    "\n",
    "            y_MNB.append(max(prob.items(), key=operator.itemgetter(1))[0])\n",
    "        result_summary[data_file][\"Laplacian\"]+=(1-((y_test != y_MNB).sum()/(y_test.shape[0])))/(num_CV*number_of_repeats)\n",
    "        result_summary[data_file][\"Laplace_time\"]=t.stop()/num_CV\n",
    "\n",
    "\n",
    "\n",
    "    winner_optimal = [result_summary[data_file][\"Gaussian\"],\n",
    "              result_summary[data_file][\"Laplacian\"],\n",
    "              result_summary[data_file][\"kNN\"],\n",
    "              result_summary[data_file][\"MNB_optimal\"]]\n",
    "    \n",
    "    winner_fixed = [result_summary[data_file][\"Gaussian\"],\n",
    "          result_summary[data_file][\"Laplacian\"],\n",
    "          result_summary[data_file][\"kNN_20\"],\n",
    "          result_summary[data_file][\"MNB_20\"],\n",
    "          result_summary[data_file][\"MNB_60\"]]\n",
    "    \n",
    "    result_summary[data_file]['Winner'] = winner_optimal.index(max(winner_optimal))\n",
    "    result_summary[data_file]['Winner_fixed'] = winner_fixed.index(max(winner_fixed))\n",
    "    print(result_summary[data_file])\n",
    "    return result_summary\n",
    "    \n",
    "\n",
    "\n",
    "result_summary={}\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LOCATION='FILE_LOCATION'\n",
    "files = ['FILE_NAME']\n",
    "\n",
    "\n",
    "for data_file in files:  \n",
    "    print(data_file)\n",
    "    \n",
    "    naive_bayes_analysis('{}/{}'.format(LOCATION,data_file),result_summary,\n",
    "                         final_features =  ['10', '1', '4', '6', '2', '9', '8', '0'],\n",
    "     \n",
    "                         kappa=42)\n",
    "\n",
    "#     print(result_summary)\n",
    "for result in result_summary:\n",
    "    print(result)\n",
    "\n",
    "keys=None\n",
    "for result in result_summary:\n",
    "    keys = result_summary[result].keys()\n",
    "\n",
    "\n",
    "with open('MNB_bank', 'w') as f: \n",
    "    w = csv.DictWriter(f, keys)\n",
    "    w.writeheader\n",
    "    for result in result_summary:\n",
    "        w.writerow(result_summary[result])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
